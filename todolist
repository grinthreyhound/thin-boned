//2 valid approaches: conditional poisson regression or inserting the matching set into poisson as ranom effect variable.
//though likely that accounting for matching will make little difference so can do sensitivity analysis to that effect
//Using own date confirmed as the right thing to do.  
//NB data definitely overdispersed, so negative binomial (xtpoisson) appropriate. 

//get the data
//start with a small section
*set memory 1g
cd "H:\JOE\THIN"

//create a smaller file:
*use ""H:\JOE\THIN\15m_3w_ss2.dta""
//drop some blank/unneeded columns:
*drop  medcode_p1 prescomplaint1 medcode_p2 prescomplaint2 medcode_s1 symptom1 medcode_s2 symptom2 medcode_s3 symptom3 medcode_s4 symptom4 medcode_s5 symptom5 medcode_s6 symptom6 medcode_s7 symptom7 medcode_s8 symptom8 medcode_e1 medcode_e1 examination1 medcode_e2 examination2 medcode_e3 examination3 medcode_e4 examination4 medcode_d1 diagnosis1 medcode_d2 diagnosis2 medcode_d3 diagnosis3 medcode_d4 diagnosis4 medcode_d5 diagnosis5 medcode_d6 diagnosis6 medcode_d7 diagnosis7 medcode_d8 diagnosis8 medcode_i1 intervention1 medcode_i2 intervention2 medcode_i3 intervention3 medcode_i4 intervention4 medcode_i5 intervention5 medcode_i6 intervention6 medcode_i7 intervention7 medcode_i8 intervention8 medcode_i9 intervention9 medcode_i10 intervention10 medcode_i11 intervention11 medcode_m1 management1 medcode_m2 management2 medcode_m3 management3 medcode_m4 management4 medcode_m5 management5 medcode_m6 management6 medcode_m7 management7 medcode_m8 management8 medcode_m9 management9
 // for the 15 month data case is 1, control is 2, recode this:
*recode case_control (2=0)
*save "H:\JOE\THIN\consult15CatsSimple.dta"

use "H:\JOE\THIN\15m_3w_ss2.dta"
drop category
//there are some missing bits - case_id and indexdate are not there for the newly added consultations
by id, sort: replace case_id = case_id[_n+1] if case_id==""
by id, sort: replace case_id = case_id[_n+1] if case_id==""
by id, sort: replace case_id = case_id[_n+1] if case_id==""

by id, sort: replace indexdate = indexdate[_n+1] if indexdate==.
by id, sort: replace indexdate = indexdate[_n+1] if indexdate==.
by id, sort: replace indexdate = indexdate[_n+1] if indexdate==.

// create new index date: where it is the last contact in the record for controls only
//(because of the looseness of matching in time +/- 3 weeks) 

by id, sort: egen lasttime = max(eventdate)
format lasttime %td
replace indexdate = lasttime if case_control==0

//Drop events that occured on the matching date:
drop if eventdate==indexdate

//drop any entries more than 3 months before the new indexdate
gen timebeforeindex = indexdate-eventdate
drop if timebeforeindex >91

//create a new entry date
gen enter = indexdate-91
format enter %td

//For the whole dataset only: check duration of time in study with events occuring: 15 month data 456 days,3 month data 90 days
*by id, sort: egen firstdate = min(eventdate)
*gen duration = lasttime - firstdate
*sum duration

//and new scaling for events: time before lasttime
*gen studyeventdate= eventdate - lasttime
*replace studyeventdate = sqrt(studyeventdate^2)

//need a variable encoding failure type for each line instead of the wise way it is represented:
generate failtype =.
replace failtype=1 if consult_group1==1 
//i.e failtype 1 = face to face consultation
replace failtype=2 if  consult_group2==1 &  consult_group1==. 
//failytype 2 = phone consult
replace failtype=4 if  consult_group4==1  
//failtype 4 = OOH consult/advice


//need to compare baseline contacts between cases and controls: 
//stset the data to indicate it is survival time data with multiple lines per person (specify id)
//Need to specify exit or else it terminates after first failure

//there are duplicates and need to work out where these came from: ?genuine or not
//drop them for now but eill need to revisit this. Looks like there are duplicates on index and non index dates. ?type
duplicates drop id eventdate, force

//need to convert snapshot data to survival time data: reshape to wide then can add variables and reshape back to long... 

//need to number the events
sort id eventdate
by id: generate n1 = _n
//drop some we don't need
drop consult_group1 consult_group2 consult_group3 consult_group4 consultid
//reshape the data
reshape wide eventdate weight weight_centile BMI height height_centile case_id locate timebeforeindex failtype , i(id) j(n1)
//add in an entrydate: make the fail type be 0
generate eventdate0=indexdate-91 
generate failtype0=0
//add in another eventdate for final index date (for n=15)
generate eventdate16=.
generate failtype16=.
//add in the exitdate (the indexdate, and put it in the first empty space) and make the indexdate failure ==99
foreach i of num 16/2 {
 local j = `i'-1
replace eventdate`i' = indexdate if eventdate`j'!=. & eventdate`i'==.
replace failtype`i' = 99 if failtype`j'!=. & failtype`i'==.
}

*preserve

//now lets reshape the data to be long...
reshape long eventdate weight weight_centile BMI height height_centile case_id locate timebeforeindex failtype, i(id) j(J)
//drop lines without data
drop if eventdate==.
format eventdate %td

*preserve
stset eventdate, id(id) fail(failtype==0 1 2 4 99) exit(time .) origin(enter) enter(enter) scale(1)
stsplit earlylate, at(7(7)91) after(enter) 
stset, clear
//Need to turn earlylate into an ordinal thingy of weeks before index
recode earlylate (0=13) (7=12) (14=11) (21=10) (28=9) (35=8) (42=7) (49=6) (56=5) (63=4) (70=3) (77=2) (84=1) 

//need a matching variable - for matched case/control groups: strings not allowed, so number them: (can't do this before stsetting as doesn't expand clusterid)
sort id
by id: generate mgroup = id if case_control ==1
replace mgroup = case_id if case_control ==0
egen clusterid = group(mgroup)

egen ngroup = max(clusterid), by(id) 

//to use Xt poisson need to xtset the data: delare it to be panel data: xtset panelvar timevar
//xtsetting is a new command, //Panel data assumes there have been observations I think, so STset/stsplit first
//cant use string id so need to group this  NB: if want to keep labels could instead use : encode id,gen(numid) 
egen numid =group(id)

//create failures from the failtype
*gen failure=1 if failtype==1 | failtype==2 | failtype==4
*gen failure=1 if failtype==1
*gen failure=1 if failtype==2
gen failure=1 if failtype==4

//Make failure missing values ==0
recode failure(.=0)

*preserve
//now need to sort out multiples: casued by some presenting >1 time in stplit period (including >1 per day for some)
//generate dup to count duplicates (of each event type, not total events in the time period!!)
sort numid earlylate failure
quietly by numid earlylate failure: gen dup = cond(_N==1,0,_n) 
//get the number of events in each duplicated time period (but not if failure!=1)
by numid earlylate failure: egen events = max(dup) if failure==1
//drop duplicates in rows
drop if dup>1 
drop if earlylate==.

//now redefine the failures to be the number of events: failure is now the number of events in the time period
replace failure = events if events>failure
recode failure (.=0)

//now drop the duplicates of id eventdate where failure =0 
sort numid earlylate
quietly by numid earlylate:gen dup1 = cond(_N==1,0,_n)

drop if dup1>0 & failure==0
//check no duplicates and sensecheck earlylate
duplicates report id earlylate
tab earlylate
tab failure
tab earlylate failure

//now we have panel data with time from event in earlylate and number of events in each time period in failure

//Inform stata is panel data, with ngroup 
xtset ngroup


//for each time period:
foreach i of num 1/13 {
preserve
display `i'
keep if earlylate==`i'
xtpoisson failure case_control, irr iter(30)
restore
}





